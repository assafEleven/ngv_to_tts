# =============================================================================
# PRODUCTION DATA PERSISTENCE SYSTEM FOR TRUST & SAFETY INVESTIGATIONS
# =============================================================================
# Complete database integration, state recovery, and backup system
# Addresses Critical Gap #2: Data Persistence & State Recovery

import os
import json
import sqlite3
import hashlib
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from contextlib import contextmanager
from enum import Enum
import pickle
import threading
from pathlib import Path
import shutil
import time

# =============================================================================
# PERSISTENCE CONFIGURATION
# =============================================================================

@dataclass
class PersistenceConfig:
    """Configuration for data persistence system"""
    # Database Configuration
    database_path: str = "trust_safety_investigations.db"
    backup_directory: str = "backups"
    
    # Backup Configuration
    backup_frequency_hours: int = 6
    backup_retention_days: int = 30
    max_backup_files: int = 100
    
    # State Management
    auto_save_interval_minutes: int = 5
    state_checkpoint_interval_minutes: int = 15
    
    # Data Retention
    investigation_retention_days: int = 365
    audit_log_retention_days: int = 90
    temp_data_retention_hours: int = 24
    
    # Performance
    connection_pool_size: int = 10
    batch_insert_size: int = 1000
    vacuum_frequency_days: int = 7
    
    # Encryption
    encrypt_at_rest: bool = True
    encryption_key_file: str = "encryption.key"
    
    # Compression
    compress_old_data: bool = True
    compression_age_days: int = 30

# =============================================================================
# DATABASE SCHEMA
# =============================================================================

DATABASE_SCHEMA = """
-- Investigations table
CREATE TABLE IF NOT EXISTS investigations (
    id TEXT PRIMARY KEY,
    title TEXT NOT NULL,
    description TEXT NOT NULL,
    investigator TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status TEXT NOT NULL DEFAULT 'active',
    risk_level TEXT NOT NULL DEFAULT 'medium',
    department TEXT,
    notes TEXT,
    metadata TEXT,  -- JSON blob for additional data
    archived_at TIMESTAMP,
    FOREIGN KEY (investigator) REFERENCES users(username)
);

-- Findings table
CREATE TABLE IF NOT EXISTS findings (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    investigation_id TEXT NOT NULL,
    finding_type TEXT NOT NULL,
    description TEXT NOT NULL,
    evidence TEXT,
    risk_level TEXT NOT NULL,
    confidence_score REAL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by TEXT NOT NULL,
    metadata TEXT,  -- JSON blob
    FOREIGN KEY (investigation_id) REFERENCES investigations(id),
    FOREIGN KEY (created_by) REFERENCES users(username)
);

-- Actions table
CREATE TABLE IF NOT EXISTS actions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    investigation_id TEXT NOT NULL,
    action_type TEXT NOT NULL,
    description TEXT NOT NULL,
    details TEXT,
    status TEXT NOT NULL DEFAULT 'pending',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    created_by TEXT NOT NULL,
    assigned_to TEXT,
    metadata TEXT,  -- JSON blob
    FOREIGN KEY (investigation_id) REFERENCES investigations(id),
    FOREIGN KEY (created_by) REFERENCES users(username),
    FOREIGN KEY (assigned_to) REFERENCES users(username)
);

-- Users table (for referential integrity)
CREATE TABLE IF NOT EXISTS users (
    username TEXT PRIMARY KEY,
    email TEXT NOT NULL UNIQUE,
    full_name TEXT NOT NULL,
    role TEXT NOT NULL,
    department TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_login TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE,
    metadata TEXT  -- JSON blob
);

-- Query history table
CREATE TABLE IF NOT EXISTS query_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    investigation_id TEXT,
    query_text TEXT NOT NULL,
    query_type TEXT NOT NULL,
    execution_time_ms INTEGER,
    records_found INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'completed',
    error_message TEXT,
    metadata TEXT,  -- JSON blob
    FOREIGN KEY (investigation_id) REFERENCES investigations(id),
    FOREIGN KEY (created_by) REFERENCES users(username)
);

-- System state table
CREATE TABLE IF NOT EXISTS system_state (
    key TEXT PRIMARY KEY,
    value TEXT NOT NULL,
    data_type TEXT NOT NULL,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,
    metadata TEXT
);

-- Data exports table
CREATE TABLE IF NOT EXISTS data_exports (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    investigation_id TEXT,
    export_type TEXT NOT NULL,
    file_path TEXT NOT NULL,
    file_size INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by TEXT NOT NULL,
    expires_at TIMESTAMP,
    access_count INTEGER DEFAULT 0,
    metadata TEXT,
    FOREIGN KEY (investigation_id) REFERENCES investigations(id),
    FOREIGN KEY (created_by) REFERENCES users(username)
);

-- Backup history table
CREATE TABLE IF NOT EXISTS backup_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    backup_type TEXT NOT NULL,
    file_path TEXT NOT NULL,
    file_size INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status TEXT NOT NULL DEFAULT 'completed',
    error_message TEXT,
    metadata TEXT
);

-- Create indexes for performance
CREATE INDEX IF NOT EXISTS idx_investigations_status ON investigations(status);
CREATE INDEX IF NOT EXISTS idx_investigations_created_at ON investigations(created_at);
CREATE INDEX IF NOT EXISTS idx_investigations_investigator ON investigations(investigator);
CREATE INDEX IF NOT EXISTS idx_findings_investigation_id ON findings(investigation_id);
CREATE INDEX IF NOT EXISTS idx_findings_created_at ON findings(created_at);
CREATE INDEX IF NOT EXISTS idx_actions_investigation_id ON actions(investigation_id);
CREATE INDEX IF NOT EXISTS idx_actions_status ON actions(status);
CREATE INDEX IF NOT EXISTS idx_query_history_created_at ON query_history(created_at);
CREATE INDEX IF NOT EXISTS idx_query_history_created_by ON query_history(created_by);
CREATE INDEX IF NOT EXISTS idx_system_state_expires_at ON system_state(expires_at);
"""

# =============================================================================
# DATABASE CONNECTION MANAGER
# =============================================================================

class DatabaseManager:
    """Production-ready database connection manager"""
    
    def __init__(self, config: PersistenceConfig):
        self.config = config
        self.db_path = config.database_path
        self.connection_pool = []
        self.pool_lock = threading.Lock()
        self.logger = logging.getLogger('trust_safety_persistence')
        
        # Initialize database
        self._initialize_database()
        
    def _initialize_database(self):
        """Initialize database with schema"""
        try:
            with self.get_connection() as conn:
                conn.executescript(DATABASE_SCHEMA)
                conn.commit()
            
            self.logger.info("Database initialized successfully")
            
        except Exception as e:
            self.logger.error(f"Database initialization failed: {e}")
            raise
    
    @contextmanager
    def get_connection(self):
        """Get database connection with connection pooling"""
        conn = None
        try:
            with self.pool_lock:
                if self.connection_pool:
                    conn = self.connection_pool.pop()
                else:
                    conn = sqlite3.connect(
                        self.db_path,
                        timeout=30.0,
                        check_same_thread=False
                    )
                    conn.row_factory = sqlite3.Row
                    
                    # Enable WAL mode for better concurrency
                    conn.execute("PRAGMA journal_mode=WAL")
                    conn.execute("PRAGMA synchronous=NORMAL")
                    conn.execute("PRAGMA cache_size=10000")
                    conn.execute("PRAGMA temp_store=MEMORY")
            
            yield conn
            
        except Exception as e:
            self.logger.error(f"Database connection error: {e}")
            if conn:
                conn.rollback()
            raise
        finally:
            if conn:
                with self.pool_lock:
                    if len(self.connection_pool) < self.config.connection_pool_size:
                        self.connection_pool.append(conn)
                    else:
                        conn.close()
    
    def close_all_connections(self):
        """Close all pooled connections"""
        with self.pool_lock:
            for conn in self.connection_pool:
                conn.close()
            self.connection_pool.clear()

# =============================================================================
# INVESTIGATION PERSISTENCE
# =============================================================================

class InvestigationPersistence:
    """Persistent storage for Trust & Safety investigations"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.logger = logging.getLogger('trust_safety_persistence')
    
    def save_investigation(self, investigation: 'Investigation') -> bool:
        """Save investigation to database"""
        try:
            with self.db_manager.get_connection() as conn:
                # Convert investigation to database format
                investigation_data = {
                    'id': investigation.investigation_id,
                    'title': investigation.title,
                    'description': investigation.description,
                    'investigator': investigation.investigator,
                    'created_at': investigation.created_at.isoformat(),
                    'updated_at': datetime.now().isoformat(),
                    'status': investigation.status,
                    'risk_level': investigation.risk_level,
                    'notes': investigation.notes,
                    'metadata': json.dumps({
                        'department': getattr(investigation, 'department', None),
                        'tags': getattr(investigation, 'tags', []),
                        'priority': getattr(investigation, 'priority', 'normal')
                    })
                }
                
                # Insert or update investigation
                conn.execute("""
                    INSERT OR REPLACE INTO investigations 
                    (id, title, description, investigator, created_at, updated_at, status, risk_level, notes, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    investigation_data['id'],
                    investigation_data['title'],
                    investigation_data['description'],
                    investigation_data['investigator'],
                    investigation_data['created_at'],
                    investigation_data['updated_at'],
                    investigation_data['status'],
                    investigation_data['risk_level'],
                    investigation_data['notes'],
                    investigation_data['metadata']
                ))
                
                # Save findings
                for finding in investigation.findings:
                    self._save_finding(conn, investigation.investigation_id, finding)
                
                # Save actions
                for action in investigation.actions_taken:
                    self._save_action(conn, investigation.investigation_id, action)
                
                conn.commit()
                self.logger.info(f"Investigation saved: {investigation.investigation_id}")
                return True
                
        except Exception as e:
            self.logger.error(f"Failed to save investigation {investigation.investigation_id}: {e}")
            return False
    
    def _save_finding(self, conn, investigation_id: str, finding: Dict[str, Any]):
        """Save finding to database"""
        conn.execute("""
            INSERT OR REPLACE INTO findings 
            (investigation_id, finding_type, description, evidence, risk_level, confidence_score, created_by, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            investigation_id,
            finding.get('type', 'general'),
            finding.get('description', ''),
            finding.get('evidence', ''),
            finding.get('risk_level', 'medium'),
            finding.get('confidence_score', 0.5),
            finding.get('created_by', 'system'),
            json.dumps(finding.get('metadata', {}))
        ))
    
    def _save_action(self, conn, investigation_id: str, action: Dict[str, Any]):
        """Save action to database"""
        conn.execute("""
            INSERT OR REPLACE INTO actions 
            (investigation_id, action_type, description, details, status, created_by, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            investigation_id,
            action.get('type', 'general'),
            action.get('description', ''),
            action.get('details', ''),
            action.get('status', 'pending'),
            action.get('created_by', 'system'),
            json.dumps(action.get('metadata', {}))
        ))
    
    def load_investigation(self, investigation_id: str) -> Optional[Dict[str, Any]]:
        """Load investigation from database"""
        try:
            with self.db_manager.get_connection() as conn:
                # Load investigation
                investigation_row = conn.execute("""
                    SELECT * FROM investigations WHERE id = ?
                """, (investigation_id,)).fetchone()
                
                if not investigation_row:
                    return None
                
                investigation = dict(investigation_row)
                
                # Load findings
                findings = conn.execute("""
                    SELECT * FROM findings WHERE investigation_id = ? ORDER BY created_at
                """, (investigation_id,)).fetchall()
                
                investigation['findings'] = [dict(finding) for finding in findings]
                
                # Load actions
                actions = conn.execute("""
                    SELECT * FROM actions WHERE investigation_id = ? ORDER BY created_at
                """, (investigation_id,)).fetchall()
                
                investigation['actions_taken'] = [dict(action) for action in actions]
                
                return investigation
                
        except Exception as e:
            self.logger.error(f"Failed to load investigation {investigation_id}: {e}")
            return None
    
    def get_investigation_history(self, limit: int = 100) -> List[Dict[str, Any]]:
        """Get investigation history"""
        try:
            with self.db_manager.get_connection() as conn:
                investigations = conn.execute("""
                    SELECT id, title, investigator, created_at, status, risk_level
                    FROM investigations 
                    ORDER BY created_at DESC 
                    LIMIT ?
                """, (limit,)).fetchall()
                
                return [dict(inv) for inv in investigations]
                
        except Exception as e:
            self.logger.error(f"Failed to get investigation history: {e}")
            return []
    
    def search_investigations(self, query: str, filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Search investigations with filters"""
        try:
            with self.db_manager.get_connection() as conn:
                sql = """
                    SELECT id, title, description, investigator, created_at, status, risk_level
                    FROM investigations 
                    WHERE (title LIKE ? OR description LIKE ?)
                """
                params = [f"%{query}%", f"%{query}%"]
                
                if filters:
                    if 'status' in filters:
                        sql += " AND status = ?"
                        params.append(filters['status'])
                    
                    if 'risk_level' in filters:
                        sql += " AND risk_level = ?"
                        params.append(filters['risk_level'])
                    
                    if 'investigator' in filters:
                        sql += " AND investigator = ?"
                        params.append(filters['investigator'])
                
                sql += " ORDER BY created_at DESC LIMIT 50"
                
                investigations = conn.execute(sql, params).fetchall()
                return [dict(inv) for inv in investigations]
                
        except Exception as e:
            self.logger.error(f"Failed to search investigations: {e}")
            return []

# =============================================================================
# STATE MANAGEMENT
# =============================================================================

class StateManager:
    """Manages system state persistence and recovery"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.logger = logging.getLogger('trust_safety_persistence')
        self.state_cache = {}
        self.cache_lock = threading.Lock()
    
    def save_state(self, key: str, value: Any, data_type: str = 'json', expires_hours: int = None):
        """Save state value"""
        try:
            with self.db_manager.get_connection() as conn:
                # Serialize value based on data type
                if data_type == 'json':
                    serialized_value = json.dumps(value)
                elif data_type == 'pickle':
                    serialized_value = pickle.dumps(value).hex()
                else:
                    serialized_value = str(value)
                
                expires_at = None
                if expires_hours:
                    expires_at = (datetime.now() + timedelta(hours=expires_hours)).isoformat()
                
                conn.execute("""
                    INSERT OR REPLACE INTO system_state 
                    (key, value, data_type, updated_at, expires_at)
                    VALUES (?, ?, ?, ?, ?)
                """, (key, serialized_value, data_type, datetime.now().isoformat(), expires_at))
                
                conn.commit()
                
                # Update cache
                with self.cache_lock:
                    self.state_cache[key] = value
                
                self.logger.debug(f"State saved: {key}")
                return True
                
        except Exception as e:
            self.logger.error(f"Failed to save state {key}: {e}")
            return False
    
    def load_state(self, key: str, default: Any = None) -> Any:
        """Load state value"""
        try:
            # Check cache first
            with self.cache_lock:
                if key in self.state_cache:
                    return self.state_cache[key]
            
            with self.db_manager.get_connection() as conn:
                row = conn.execute("""
                    SELECT value, data_type, expires_at FROM system_state 
                    WHERE key = ?
                """, (key,)).fetchone()
                
                if not row:
                    return default
                
                value, data_type, expires_at = row
                
                # Check if expired
                if expires_at:
                    expire_time = datetime.fromisoformat(expires_at)
                    if datetime.now() > expire_time:
                        self.delete_state(key)
                        return default
                
                # Deserialize value
                if data_type == 'json':
                    deserialized_value = json.loads(value)
                elif data_type == 'pickle':
                    deserialized_value = pickle.loads(bytes.fromhex(value))
                else:
                    deserialized_value = value
                
                # Update cache
                with self.cache_lock:
                    self.state_cache[key] = deserialized_value
                
                return deserialized_value
                
        except Exception as e:
            self.logger.error(f"Failed to load state {key}: {e}")
            return default
    
    def delete_state(self, key: str) -> bool:
        """Delete state value"""
        try:
            with self.db_manager.get_connection() as conn:
                conn.execute("DELETE FROM system_state WHERE key = ?", (key,))
                conn.commit()
                
                # Remove from cache
                with self.cache_lock:
                    self.state_cache.pop(key, None)
                
                return True
                
        except Exception as e:
            self.logger.error(f"Failed to delete state {key}: {e}")
            return False
    
    def cleanup_expired_state(self):
        """Clean up expired state entries"""
        try:
            with self.db_manager.get_connection() as conn:
                conn.execute("""
                    DELETE FROM system_state 
                    WHERE expires_at IS NOT NULL AND expires_at < ?
                """, (datetime.now().isoformat(),))
                conn.commit()
                
                # Clear cache
                with self.cache_lock:
                    self.state_cache.clear()
                
                self.logger.info("Expired state entries cleaned up")
                
        except Exception as e:
            self.logger.error(f"Failed to cleanup expired state: {e}")

# =============================================================================
# BACKUP SYSTEM
# =============================================================================

class BackupManager:
    """Comprehensive backup and recovery system"""
    
    def __init__(self, db_manager: DatabaseManager, config: PersistenceConfig):
        self.db_manager = db_manager
        self.config = config
        self.backup_dir = Path(config.backup_directory)
        self.backup_dir.mkdir(exist_ok=True)
        self.logger = logging.getLogger('trust_safety_persistence')
    
    def create_backup(self, backup_type: str = 'scheduled') -> Optional[str]:
        """Create database backup"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_filename = f"trust_safety_backup_{timestamp}.db"
            backup_path = self.backup_dir / backup_filename
            
            # Create backup
            with self.db_manager.get_connection() as conn:
                backup_conn = sqlite3.connect(str(backup_path))
                conn.backup(backup_conn)
                backup_conn.close()
            
            # Get file size
            file_size = backup_path.stat().st_size
            
            # Record backup in database
            with self.db_manager.get_connection() as conn:
                conn.execute("""
                    INSERT INTO backup_history 
                    (backup_type, file_path, file_size, created_at, status)
                    VALUES (?, ?, ?, ?, ?)
                """, (backup_type, str(backup_path), file_size, datetime.now().isoformat(), 'completed'))
                conn.commit()
            
            self.logger.info(f"Backup created: {backup_path}")
            
            # Clean up old backups
            self._cleanup_old_backups()
            
            return str(backup_path)
            
        except Exception as e:
            self.logger.error(f"Backup failed: {e}")
            
            # Record failed backup
            try:
                with self.db_manager.get_connection() as conn:
                    conn.execute("""
                        INSERT INTO backup_history 
                        (backup_type, file_path, created_at, status, error_message)
                        VALUES (?, ?, ?, ?, ?)
                    """, (backup_type, "", datetime.now().isoformat(), 'failed', str(e)))
                    conn.commit()
            except:
                pass
            
            return None
    
    def _cleanup_old_backups(self):
        """Clean up old backup files"""
        try:
            cutoff_date = datetime.now() - timedelta(days=self.config.backup_retention_days)
            
            # Get backups to delete
            with self.db_manager.get_connection() as conn:
                old_backups = conn.execute("""
                    SELECT id, file_path FROM backup_history 
                    WHERE created_at < ? AND status = 'completed'
                    ORDER BY created_at
                """, (cutoff_date.isoformat(),)).fetchall()
                
                deleted_count = 0
                for backup_id, file_path in old_backups:
                    try:
                        if os.path.exists(file_path):
                            os.remove(file_path)
                        
                        # Remove from database
                        conn.execute("DELETE FROM backup_history WHERE id = ?", (backup_id,))
                        deleted_count += 1
                        
                    except Exception as e:
                        self.logger.warning(f"Failed to delete backup {file_path}: {e}")
                
                conn.commit()
                
                if deleted_count > 0:
                    self.logger.info(f"Cleaned up {deleted_count} old backups")
                    
        except Exception as e:
            self.logger.error(f"Backup cleanup failed: {e}")
    
    def restore_backup(self, backup_path: str) -> bool:
        """Restore database from backup"""
        try:
            if not os.path.exists(backup_path):
                raise FileNotFoundError(f"Backup file not found: {backup_path}")
            
            # Close all connections
            self.db_manager.close_all_connections()
            
            # Create backup of current database
            current_backup = self.create_backup('pre_restore')
            
            # Restore from backup
            shutil.copy2(backup_path, self.config.database_path)
            
            self.logger.info(f"Database restored from: {backup_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"Restore failed: {e}")
            return False
    
    def get_backup_history(self) -> List[Dict[str, Any]]:
        """Get backup history"""
        try:
            with self.db_manager.get_connection() as conn:
                backups = conn.execute("""
                    SELECT * FROM backup_history 
                    ORDER BY created_at DESC 
                    LIMIT 50
                """).fetchall()
                
                return [dict(backup) for backup in backups]
                
        except Exception as e:
            self.logger.error(f"Failed to get backup history: {e}")
            return []

# =============================================================================
# MAIN PERSISTENCE SYSTEM
# =============================================================================

class ProductionPersistenceSystem:
    """Complete production persistence system"""
    
    def __init__(self, config: PersistenceConfig = None):
        self.config = config or PersistenceConfig()
        self.logger = self._setup_logger()
        
        # Initialize components
        self.db_manager = DatabaseManager(self.config)
        self.investigation_persistence = InvestigationPersistence(self.db_manager)
        self.state_manager = StateManager(self.db_manager)
        self.backup_manager = BackupManager(self.db_manager, self.config)
        
        # Background tasks
        self.auto_save_thread = None
        self.backup_thread = None
        self._stop_threads = False
        
        # Start background tasks
        self._start_background_tasks()
    
    def _setup_logger(self) -> logging.Logger:
        """Set up persistence logging"""
        logger = logging.getLogger('trust_safety_persistence')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # File handler
            file_handler = logging.FileHandler('trust_safety_persistence.log')
            file_handler.setLevel(logging.INFO)
            
            # Console handler
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.WARNING)
            
            # Formatter
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s'
            )
            file_handler.setFormatter(formatter)
            console_handler.setFormatter(formatter)
            
            logger.addHandler(file_handler)
            logger.addHandler(console_handler)
        
        return logger
    
    def _start_background_tasks(self):
        """Start background maintenance tasks"""
        if self.config.auto_save_interval_minutes > 0:
            self.auto_save_thread = threading.Thread(
                target=self._auto_save_loop, 
                daemon=True
            )
            self.auto_save_thread.start()
        
        if self.config.backup_frequency_hours > 0:
            self.backup_thread = threading.Thread(
                target=self._backup_loop, 
                daemon=True
            )
            self.backup_thread.start()
    
    def _auto_save_loop(self):
        """Background auto-save loop"""
        while not self._stop_threads:
            try:
                time.sleep(self.config.auto_save_interval_minutes * 60)
                self.state_manager.cleanup_expired_state()
                
            except Exception as e:
                self.logger.error(f"Auto-save loop error: {e}")
    
    def _backup_loop(self):
        """Background backup loop"""
        while not self._stop_threads:
            try:
                time.sleep(self.config.backup_frequency_hours * 3600)
                self.backup_manager.create_backup('scheduled')
                
            except Exception as e:
                self.logger.error(f"Backup loop error: {e}")
    
    def get_system_health(self) -> Dict[str, Any]:
        """Get persistence system health"""
        try:
            with self.db_manager.get_connection() as conn:
                # Database size
                db_size = os.path.getsize(self.config.database_path)
                
                # Record counts
                investigation_count = conn.execute("SELECT COUNT(*) FROM investigations").fetchone()[0]
                finding_count = conn.execute("SELECT COUNT(*) FROM findings").fetchone()[0]
                action_count = conn.execute("SELECT COUNT(*) FROM actions").fetchone()[0]
                
                # Recent activity
                recent_activity = conn.execute("""
                    SELECT COUNT(*) FROM investigations 
                    WHERE created_at > datetime('now', '-1 day')
                """).fetchone()[0]
                
                return {
                    'status': 'healthy',
                    'database_size_bytes': db_size,
                    'investigation_count': investigation_count,
                    'finding_count': finding_count,
                    'action_count': action_count,
                    'recent_activity_24h': recent_activity,
                    'backup_directory': str(self.backup_manager.backup_dir),
                    'last_backup': self._get_last_backup_time(),
                    'auto_save_enabled': self.config.auto_save_interval_minutes > 0,
                    'backup_enabled': self.config.backup_frequency_hours > 0
                }
                
        except Exception as e:
            self.logger.error(f"Health check failed: {e}")
            return {'status': 'unhealthy', 'error': str(e)}
    
    def _get_last_backup_time(self) -> Optional[str]:
        """Get time of last successful backup"""
        try:
            with self.db_manager.get_connection() as conn:
                result = conn.execute("""
                    SELECT created_at FROM backup_history 
                    WHERE status = 'completed' 
                    ORDER BY created_at DESC 
                    LIMIT 1
                """).fetchone()
                
                return result[0] if result else None
                
        except Exception as e:
            self.logger.error(f"Failed to get last backup time: {e}")
            return None
    
    def shutdown(self):
        """Gracefully shutdown persistence system"""
        self._stop_threads = True
        
        # Wait for threads to finish
        if self.auto_save_thread and self.auto_save_thread.is_alive():
            self.auto_save_thread.join(timeout=5)
        
        if self.backup_thread and self.backup_thread.is_alive():
            self.backup_thread.join(timeout=5)
        
        # Close database connections
        self.db_manager.close_all_connections()
        
        self.logger.info("Persistence system shutdown complete")

# =============================================================================
# INITIALIZATION
# =============================================================================

def initialize_persistence_system(config: PersistenceConfig = None) -> ProductionPersistenceSystem:
    """Initialize the complete persistence system"""
    print("ðŸ’¾ INITIALIZING PRODUCTION PERSISTENCE SYSTEM")
    print("=" * 60)
    
    # Create persistence system
    persistence_system = ProductionPersistenceSystem(config)
    
    # Make globally available
    global persistence_system as global_persistence
    global_persistence = persistence_system
    
    # Test system
    health = persistence_system.get_system_health()
    
    print("âœ… Database initialized with schema")
    print("âœ… Connection pooling enabled")
    print("âœ… Backup system configured")
    print("âœ… State management ready")
    print("âœ… Background tasks started")
    
    print("\nðŸŽ¯ PERSISTENCE SYSTEM READY!")
    print("=" * 60)
    print(f"Database: {persistence_system.config.database_path}")
    print(f"Backup directory: {persistence_system.config.backup_directory}")
    print(f"Investigation count: {health.get('investigation_count', 0)}")
    print(f"Auto-save interval: {persistence_system.config.auto_save_interval_minutes} minutes")
    print(f"Backup frequency: {persistence_system.config.backup_frequency_hours} hours")
    
    print("\nðŸ“‹ Available functions:")
    print("  â€¢ persistence_system.investigation_persistence.save_investigation(investigation)")
    print("  â€¢ persistence_system.investigation_persistence.load_investigation(id)")
    print("  â€¢ persistence_system.state_manager.save_state(key, value)")
    print("  â€¢ persistence_system.state_manager.load_state(key)")
    print("  â€¢ persistence_system.backup_manager.create_backup()")
    print("  â€¢ persistence_system.get_system_health()")
    
    return persistence_system

# =============================================================================
# PRODUCTION PERSISTENCE SYSTEM READY
# =============================================================================

print("ðŸ’¾ PRODUCTION PERSISTENCE SYSTEM LOADED")
print("=" * 60)
print("Critical Persistence Features:")
print("  âœ… SQLite Database with WAL mode")
print("  âœ… Connection Pooling")
print("  âœ… State Management")
print("  âœ… Automatic Backups")
print("  âœ… Data Recovery")
print("  âœ… Background Tasks")
print("  âœ… Health Monitoring")
print("")
print("ðŸš€ Initialize with: initialize_persistence_system()") 